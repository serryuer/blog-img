## 信息检索导论-读书笔记(3)-信息检索中的文档分类

本文是对国科大2018-2019年春季教授的信息检索课程的复习总结，主要参考《信息检索导论》一书以及老师上课所用课件。根据书中章节的划分，本系列文章为五个部分，本文是第三部分：信息检索中的文档分类，其余部分如下：

- [信息检索导论-读书笔记(1)-信息检索导论基础知识](<https://blog.csdn.net/serryuer/article/details/89811168>)
- [信息检索导论-读书笔记(2)-相关反馈和查询扩展、概率模型](#)
- [信息检索导论-读书笔记(3)-信息检索中的文档分类](#)
- [信息检索导论-读书笔记(4)-信息检索中的文档聚类](#)**待完成**
- [信息检索导论-读书笔记(5)-信息检索的应用-Web搜索](#)**待完成**

## 0. 本文概述

本文主要关注文档分类的问题。首先介绍了搜索引擎中问什么需要文档分类技术，然后介绍了几种文本分类算法，包括最简单的朴素贝叶斯分类算法、基于向量空间模型的Rocchio和kNN算法、支持向量机，并给出了文本分类的评价方法。

## 1. 文本分类及朴素贝叶斯算法

首先想一个问题，信息检索领域为什么需要文本分类技术？下面举一些信息检索领域中分类的应用的例子：

1. 垃圾网页的自动识别，这些识别出来的垃圾网页不会被搜索引擎索引；

2. 情感分析，主要用于评论的分析，比如电商平台商品的评论；

3. 面向主题的搜索或者垂直搜索，垂直搜索将搜索限制在某个主题或者领域；

4. 最后，ad hoc IR中的排序函数也可以基于文档分类器来构建。

### 1.1 文本分类问题

给定文档$d \in \mathcal{X}$和一个给定的类别集合$\mathcal{C} = \{c_1, c_2,\dots, c_j\}$，其中$\mathcal{X}$表示文档空间，类别也称为class\category\label，利用某种学习算法，我们希望学到某个分类函数$\gamma$，它可以将文档映射到类别：
$$\gamma : \mathcal{X} \rightarrow \mathcal{C}$$

### 1.2 朴素贝叶斯文本分类

我们介绍的第一个有监督的学习算法是**多项式朴素贝叶斯**或者叫**多项式NB**模型，是一种基于概率的学习算法，文档d属于类别c的概率计算如下：
$$
P(c | d) \propto P(c) \prod_{1<k \in n_{d}} P\left(t_{k} | c\right)
$$
其中，$p(t_k|c)$是$t_k$出现在类别c文档中的条件概率，$p(c)$是类别c的先验概率，根据最大后验估计，我们的目标是：
$$
c_{m a p}=\arg \max _{c \in \mathrm{C}} \hat{P}(c | d)=\arg \max _{c \in \mathrm{C}} \hat{P}(c) \prod_{1<k \in n_{d}} \hat{P}\left(t_{k} | c\right)
$$
为了避免数值下溢，我们采用对数后验概率：
$$
c_{m a p}=\arg \max _{c \in \mathrm{C}} \hat{P}(c | d)=\underset{c \in \mathrm{C}}{\arg \max }\left[\log \hat{P}(c)+\sum_{16 k \mathfrak{k} n_{d}} \log \hat{P}\left(t_{k} | c\right)\right]
$$
接下来使用MLE进行参数估计：
$$
\hat{P}(c)=\frac{N_{c}}{N}
$$
$$
\hat{P}(t | c)=\frac{T_{c t}}{\sum_{t^{\prime} \in V} T_{c t^{\prime}}^{\prime}}
$$
其中$N_c$是类别为c的文档的数量，$T_{ct}$是类别为c的文档中t的词频。
为了避免有些词项在训练集中没有出现，而导致概率为0，我们可以采用加一平滑：
$$
\hat{P}(t | c)=\frac{T_{c t}+1}{\sum_{t \in V}\left(T_{c t}+1\right)}=\frac{T_{c t}+1}{\sum_{t \in V} T_{c t}+B}
$$
加一平滑可以认为是使用均匀分布作为词项的先验分布（每个词项只出现1次）然后根据训练数据进行更新得到的结果。

算法描述如下：

<center>
<img src="https://github.com/serryuer/blog-img/raw/master/imgs/信息检索导论3/Snipaste_2019-05-04_06-06-41.png" width=50% height=50%/>
</center>

接下来我们讨论NB算法的复杂度，
<center>
<img src="https://github.com/serryuer/blog-img/raw/master/imgs/信息检索导论3/Snipaste_2019-05-04_06-10-06.png" width=50% height=50%/>   
</center>

### 1.3 贝努利模型

建立NB模型有两种不同的方法，上一节介绍的是一种基于生成式模型在文档的每个位置生成词表中的一个词项的方法，另一种方法是多元贝努利模型，等价于二值独立模型，对于词汇表中的每个词汇都对应一个二值变量，存在或者不存在，而不考虑词项出现的次数，下面是基于贝努利模型的NB算法的具体描述：
<center>
<img src="https://github.com/serryuer/blog-img/raw/master/imgs/信息检索导论3/Snipaste_2019-05-04_06-16-03.png" width=50% height=50%/>
</center>

不同的生成模型也意味着不同的参数估计策略和分类规则，先看上面的参数估计策略，贝努利模型中参数$\hat{P}(t|c)$是利用类c中包含词项t的文档数目的比例来计算，而与之形成对比的是，多项式模型中计算的是t出现的次数占类c文档中所有词项数目的比例。所以二者最根本的区别就是是否考虑同一词项在某一文档中多次出现。

### 1.4 NB的性质
为了对两种模型有更深的理解，我们从生成模型的角度来解释，下式是分类规则的最终形式：
$$
c_map=\underset{c \in C}{\arg \max } P(d | c) P(c)
$$
两种模型主要在计算条件概率时的方法不一样：
$$
\begin{array}{l}多项式模型 {P(d | c)=P\left(<t_{1}, \ldots, t_{k}, \ldots, t_{n_{d}}>| c\right)} \\ 贝努利模型 {P(d | c)=P\left(<e_{1}, \ldots, e_{i}, \ldots, e_{M}>| c\right)}\end{array}
$$
$<t_{1}, \ldots, t_{n d}> 和 <e_{1}, \ldots, e_{i}, \ldots, e_{M}>$是两种不同的文档表示方式，第一种表示方式中，文档空间是所有词序列的集合，第二种表示方式中，文档空间是$\{0,1\}^M$。

上述公式不能直接用于文本分类，因为其参数数量太大，贝努利模型的参数数量是$2^M$，多项式模型有同样数量级的参数，因此这里引入条件独立性假设，即给定类别c时，假设属性值之间是相互独立的。

但是在多项式模型中，即使采用了条件独立性假设，因此考虑了同一词项在文档不同位置出现的情况，如果它们之间的概率不一样，需要估计的参数数量仍然很多，因此引入了另外一个独立性假设，位置独立性假设，假设在不同位置上词项的分布是一致的。

总结一下两个模型生成文档的过程：
1. 多项式模型中，首先以$P(c)$来选择一个类别c，然后根据模型来生成一篇文档，对于文档的$n_d$个位置，在每个位置k上以概率$P(X_k=t_k|c)$生成词项$t_k$，这里我们还需要提前对文档的长度分布进行定义，这样才能生成一篇文档而不是一个词项序列。
2. 贝努利模型中，首先以$P(c)$来选择一个类别c，然后针对词典中的每一个词项$t_i$，都产生一个二值变量$e_i$，表示文档中是否包含该词项。、

两种模型的比较如下：
<center>
<img src="https://github.com/serryuer/blog-img/raw/master/imgs/信息检索导论3/Snipaste_2019-05-04_06-59-29.png" width=50% height=50%/>
</center>

朴素贝叶斯之所以朴素就是因为它忽略了自然语言句子中词序相关的信息，但是即使它概率估计效果很差，但是其分类效果很好，如下表所示：
<center>
<img src="https://github.com/serryuer/blog-img/raw/master/imgs/信息检索导论3/Snipaste_2019-05-04_07-01-45.png" width=50% height=50%/>
</center>

NB模型还有其他一些优点，NB能够对噪音特征和概念漂移有一定的鲁棒性，其主要优势是速度快。