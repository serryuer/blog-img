@[TOC](目录)


## 0 HNN起源

1982年，J．Hopfield提出了可用作联想存储器的互连网络，这个网络称为Hopfield网络模型，也称Hopfield模型。Hopfield神经网络模型是一种循环神经网络，从输出到输入有反馈连接。

## 1 HNN简介

神经网络根据运行过程中的信息流向，可分为**前馈式**和**反馈式**两种基本类型。前馈网络的输出仅由当前输入和权重矩阵决定，而与网络先前的输出状态无关，反馈网络会将输出再次作为输入的一部分输入到网络中，Hopfield 网络被认为是一种最典型的全反馈网络。

Hopfield 网络可以看成一种非线性的动力学系统。反馈网络能够表现出非线性动力学系统的动态特性。它所具有的主要特性为以下两点：
- 网络系统具有若干个**稳定状态**，对应了**能量函数**的局部最小值，当网络从某一初始状态开始运动，网络系统总可以收敛到某一个稳定的**平衡状态**；
- 系统稳定的平衡状态可以通过设计网络的权值而被存储到网络中；

反馈型神经网络作为非线性动态系统，表现出丰富多样的动态特性，比如稳定性、极限环、奇怪吸引子等等。
非线性系统演变的形式有以下几种：
- 渐进稳定
- 极限环
- 混沌现象
- 状态轨迹发散

经过特殊设计的能量函数，可以使得，从任意状态开始，系统的演变都会使得能量函数单调减少，从而最后稳定到最小值，下面会证明这个定理。网络的稳定性是Hopefield网络的重要性质，而能量函数是判定网络稳定性的基本概念。

可以参考**马尔科夫链及其平稳分布**，说的是任一个马尔科夫链，如果其转移矩阵$P$固定，那么不管其初始分布是什么状态，经过若干次迭代之后，都会收敛于一个平稳分布$p(x)$。马尔科夫链及其平稳分布是MCMC系列采样方法的理论基础。

有了能量函数，我们就有了每一个状态所对应的能量值，我们可以用这个能量值的大小来定义每一个状态发生的概率，即状态的概率分布函数。

根据神经元输出的值的范围，Hopefield网络又分为离散型和连续型，分别记为**DHNN**和**CHNN**，DHNN主要用于联想记忆，CHNN主要用于优化计算。

## 2 DHNN结构
首先考虑由三个神经元组成的离散Hopfield神经网络，其结构如下图所示：

<center>
<img src="https://github.com/serryuer/blog-img/raw/master/imgs/Hopefield/Snipaste_2019-04-18_21-17-04.jpg" width=80% height=80%/>

在图中，第0层仅仅是作为网络的输入，它不是实际神经元，所以无计算功能；而第一层是实际神经元，故而执行对输人信息和权系数乘积求累加和，并由非线性函数$f$处理后产生输出信息。$f$是一个简单的阈值函效，如果神经元的输出信息大于阈值$θ$，那么，神经元的输出就取值为1；小于阈值$θ$，则神经元的输出就取值为0。

对于二值神经元，它的计算公式如下：
$$U_j = \sum_iW_{ij}*Y_i + X_j$$
其中，$X_i$为外部输入，并且有：
$$
Y_j=
\begin{cases}
1,\  if\ U_j \geq \theta\\
0,\ if \ U_j < \theta    
\end{cases}
$$

对于一个离散的Hopefield网络，其网络状态是输出神经元信息的集合，对于一个输出层是$n$个神经元的网络，其在时刻$t$的状态为一个$n$维向量：
$$Y(t)=[Y_1(t), Y_2(t), ..., Y_n(t)]$$
因为每个输出神经元都是二值神经元，故而一共有$2^n$种状态，即是网络状态。该网络有$n*n$权重系数矩阵$W$，同时有$n$维阈值向量$\Theta$。
$$W={W_{ij}}\ (i={1,2,...,n}\ j={1,2,...,n})$$
$$\Theta=[\theta_1, \theta_2, ... , \theta_n]$$
当$W_{ii}=0$时说明一个神经元的输出不会反馈到它自己的输入，这时离散的Hopefield网络被称为**无自反馈**的网络，反之为**全自反馈型网络**。

对于以符号函数为激励函数的网络，网络的方程可以写成：
$$u_i(t+1)=\sum_{j=1}^nW_{ij}x_j(t)-\theta_i$$
$$x_i(t+1)=sgn(u_i(t+1))\ \ \ i=(1,2,..,.,n)$$


## 3 DHNN的学习

### 3.1 DHNN的状态变化

DHNN的学习过程有两种工作方式：
- 串行工作方式：在某一时刻只有一个神经元改变状态，其他神经元保持不变，这一变化的方式可以按照随机的方式或者按照预定的顺序来选择。
- 并行工作方式：在某一时刻有N个神经元改变状态，其他神经元保持不变，这一变化的方式可以按照随机的方式或者按照预定的顺序来选择，当$N=n$的时候，称为全并行方式。

每一个时刻整个网络处于一个状态，状态的变化采用随机异异步更新的方式，即随机的选择下一个要更新的神经元，且允许所有神经元有相同的平均变化概率，当该节点状态变化时，下一个时刻的网络转移到下一个状态，当该节点状态不变时，下一个时刻的网络状态保持不变。

HNN的稳定工作点：
$$X_i(t+1)=X_i(t)=sgn(\sum_{j=1}^nW_{ij}x_i(t) - \theta_1)\tag{4.1}$$

**稳定性判据**：当网络工作在串行方式下时，若$W$为对称阵，且其对角元素非负，则其能量函数单调下降，网络总能收敛到一个稳定点。

### 3.2 DHNN的能量函数

Hopefield网络是一个多输入、多输出、带阈值的二态非线性动力学系统，在满足一定的参数条件下，能量函数在网络运行过程中是不断降低、最后趋于稳定平衡状态的。

假设第$i$个神经元节点状态$v_i$的变化量记为$\Delta v_i$，相应的能量变化记为$\Delta E_i$，那么能量$E_i$随着状态变化单调减小意味着$\Delta E_i$一直小于等于0。

考虑两种情况（节点状态不变时，能量变化为0）：
1. 当状态由1变为0时，$\Delta v_i < 0$
2. 当状态由0变为1时，$\Delta v_i > 0$

按照能量变化量不大于0的思路，可以将能量变化量$\Delta E_i$表示成下面的形式：
$$\Delta E_i=-(\sum_{j=1}^nW_{ij}x_j-\theta _i)\Delta v_i$$
上式中，当$\Delta v_i > 0$的时候，$\sum_{j=1}^nW_{ij}x_j-\theta _i$大于0，所以能量变化小于0，反之同理。

所以节点$i$的能量可以定义为：
$$E_i=-(\sum_{j=1}^nW_{ij}v_j-\theta _i)v_i$$

当前状态的总能量为：
$$E=-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^nW_{ij}v_iv_j + \sum_{i=1}^n\theta_iv_i$$

### 3.3 DHNN稳定状态求解

只有当网络的极小点可被选择或者设定的时候，网络所具有的能力才能发挥作用，就像在MCMC采样算法中利用马尔科夫链的平稳分布特性来获取概率分布$p(n)$，我们的目的是设计一个马尔科夫链使其最终的平稳分布就是我们想要采样的分布$p(n)$。在这里，我们想要的就是网络的稳定状态，即网络的能量极小点，它是由网络的连接权值和阈值所决定的，因此我们的目标就是获取一组合适的参数值。

主要有两种方法：
- 通过规则计算得到，根据求解问题的要求直接设计出所需要的连接权值；
- 通过一定的学习算法，自动得到，所需要的权重和参数（比如Hebbe学习规则，误差学习规则）

前者为静态学习算法，对于一个具体应用而言，权值矩阵为定常矩阵，比如TSP求解。后者为动态学习算法，比如联想记忆。

如果我们能量极小点的状态，那么我们就可以根据这组状态（因为能量极小点不止一个）设计满足条件的网络参数，方法就是设计一组方程，每一个方程表示当其中一个输出神经元变化的时候，能量变化量的大小，令每一个方程都大于等于，即不管任一个神经元如何变化，总的能量都不会变小，根据一系列方程组就可以求得我们想要的参数。

需要注意的是，我们根据一组能量极小点求得的网络参数满足了这一组能量极小点的要求，但是同时也可能产生这一组之外的能量极小点。

Hopefield网络可以用于联想记忆和计算，当给定权重矩阵$W$，求网络最终的稳定状态时，其用于计算；反过来则是用于联想记忆。

Hopefield网络作为一种基于能量的模型(EBM)，同属于EBM的还有图变换网络(Graph-transformer Networks)，条件随机场，最大化边界马尔科夫网络以及一些流形学习的方法等，EBM有两个任务，一个是推断，一个是学习，我们可以和明显地看出当给定权重矩阵$W$，求网络最终的稳定状态时，就是一个**推断问题**，也叫搜索问题，即搜索最终的稳定状态；反过来给定最终的稳定状态，求网络参数，则是**学习问题**。

推断问题比较简单，根据我们设计能量函数的原则，只要我们迭代的对每一个神经元进行更新，我们总能达到一个能量的局部最小值，即平稳点，但是我们也只能得到一个局部最优解，很难得到全局最优解，这是由Hopefield网络的确定性所决定的，为了解决这个问题，可以使用**玻尔兹曼机**。

### 3.4 DHNN权值的设计方法

权重设计的方法有内积法、外积法、伪逆法、正交设计法等。

**外积法(Hebb学习规则)**：给定输入$X^K$, $K=1,2,..,m$, $X\in R^n$, $I$为$n*n$单位阵，则有：
$$W=\sum_{k=1}^m[X^K(X^K)^T-I]$$
$$w_{ij}=\sum_{i=1}^mx_i^kx_j^k$$
$$w_{ii}=0$$
按照上述规则求出权重矩阵之后，网络已经将模式存入网络的连接权中，在联想的过程中，先给出一个**原始模式**，使网络处于某种初始状态下，用网络方程动态运行，最后达到一个稳定状态，如果此稳定状态对应于网络已经存储的某一模式，则称原始模式是由该稳定状态对应的模式联想起来的。

<center>
<img src="https://github.com/serryuer/blog-img/raw/master/imgs/Hopefield/Snipaste_2019-04-18_21-22-44.jpg" width=50% height=80%/>

Hopefield网络用于联想记忆时受记忆容量（输出神经元个数）和样本差异（记忆的模式）制约，当记忆的模式较少且模式之间的差异较大时，联想结果正确率较高；当需要记忆的模式较多并且模式之间差异性不明显的时候往往得到的网络稳定状态不是已经记忆的模式。

### 3.5 DHNN的记忆容量分析

当网络只需要记忆一个模式的时候，该模式肯定被网络准确无误的记住，当模式个数增加的时候清理发生了变化，主要表现在以下两点：
1. **权值移动**
2. **交叉干扰**

观察Hebb学习规则我们可以发现，当k值较小时，其可以使记忆模式即为吸引子（能量极小点），当时当输入样本增大时，所有的样本都堆叠在权重矩阵上，会导致权重矩阵忘记最开始的记忆模式，这一现象称为"疲劳”，和RNN的长依赖、NN的梯度消失是一个道理。

网络在学习多个样本之后，在回忆阶段，即验证该样本时产生的干扰，称为交叉干扰。对Hebb学习规则而言，当输入样本时正交的时候，n个神经元的记忆上限为n，但是在大多数情况下，输入样本不可能是正交的，因为网络的记忆容量一般小得多。一般为$(0.12-0.15)n​$。

### 3.6 DHNN权重修正的其他方法

- $\delta$学习规则
- 伪逆法
- 正交化权值设计


## 4 CHNN

CHNN是在DHNN的基础上提出的，原理和DHNN相似。
//TODO

## 5 HNN的应用
Hopefield网络可以用于记忆和联想。因为，每个HopField都有特定的稳定状态，就相当于该网络对这些状态具有了记忆。从一些相关的、不全的记忆，可以触发类似联想的过程，最终稳定到某个稳定状态。非常类似人类的记忆和联想过程。

稳定性是这类具有联想记忆功能神经网络模型的核心，学习记忆的过程就是系统向稳定状态发展的过程。

### 5.1 Hopefield网络在组合优化中的应用
//TODO


## 参考文献
[DL：Hopfield 神经网络](https://blog.csdn.net/oMengLiShuiXiang1234/article/details/49531937)

[Hopfield神经网络的通俗理解](https://blog.csdn.net/changdejie/article/details/78099410)

[第10章人工神经网络-Hopfield](http://www.docin.com/p-921356154.html)

[Hopfield神经网络详解](https://blog.csdn.net/jichangzhen/article/details/78777646)

[Hopfield神经网络](https://blog.csdn.net/richard2357/article/details/23184999)

[Hopfield 网络](https://www.cnblogs.com/zhoukui/p/7737404.html)

[人工智能之Hopfield神经网络（HNN）](https://ai.ofweek.com/news/2018-05/ART-201717-11001-30228892.html)